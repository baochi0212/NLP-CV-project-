model:
  arch: img2prompt_vqa
  model_type: base

datasets:
  text_vqa: # name of the dataset builder
    type: eval
    vis_processor:
        eval:
          name: "blip_image_eval"
          image_size: 384
    text_processor:
        eval:
          name: "blip_caption"

run:
  task: textvqa_reading_comprehension

  # optimization-specific
  batch_size_train: 4
  batch_size_eval: 4
  num_workers: 1

  # image question matching specific
  block_num: 7

  # image captioning specific
  top_k: 50
  top_p: 1
  cap_min_length: 10
  cap_max_length: 20
  repetition_penalty: 1
  num_patches: 20
  num_captions: 50
  prompt: 'a picture of '

  # question answering specific
  internal_bsz_fid: 1
  num_captions_fid: 1
  min_len: 0
  max_len: 20
  num_beams: 1
  inference_method: "generate"

  seed: 42
  output_dir: "output/Img2LLM-VQA/TextVQA_val"

  evaluate: True
  test_splits: ["val"]

  # distribution-specific
  device: "cuda"
  world_size: 1
  dist_url: "env://"
  distributed: True
