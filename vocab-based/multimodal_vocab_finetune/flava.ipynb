{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8001\n",
      "['<unk>\\n', 'yes\\n', 'unanswerable\\n', 'answering does not require reading text in the image\\n', 'no\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: textvqa/textvqa\n",
      "Found cached dataset textvqa (/home/ubuntu/.cache/huggingface/datasets/textvqa/textvqa/0.5.1/9b89037cc122c3b495b155a1bce4170851829843454e88f236bb8715d977c027)\n",
      "100%|██████████| 3/3 [00:00<00:00, 459.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK index <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/vqa/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"./vocabs/answers_textvqa_8k.txt\") as f:\n",
    "  vocab = f.readlines()\n",
    "\n",
    "answer_to_idx = {}\n",
    "for idx, entry in enumerate(vocab):\n",
    "  answer_to_idx[entry.strip(\"\\n\")] = idx\n",
    "print(len(vocab))\n",
    "print(vocab[:5])\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"textvqa\")\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "from transformers import BertTokenizer\n",
    "from functools import partial\n",
    "\n",
    "def transform(tokenizer, input):\n",
    "  batch = {}\n",
    "  image_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize([224,224])])\n",
    "  image = image_transform(input[\"image\"][0].convert(\"RGB\"))\n",
    "  batch[\"image\"] = [image]\n",
    "\n",
    "  tokenized=tokenizer(input[\"question\"],return_tensors='pt',padding=\"max_length\",max_length=512)\n",
    "  batch.update(tokenized)\n",
    "\n",
    "\n",
    "  ans_to_count = defaultdict(int)\n",
    "  for ans in input[\"answers\"][0]:\n",
    "    ans_to_count[ans] += 1\n",
    "  max_value = max(ans_to_count, key=ans_to_count.get)\n",
    "  ans_idx = answer_to_idx.get(max_value,0)\n",
    "  batch[\"answers\"] = torch.as_tensor([ans_idx])\n",
    "  return batch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\",padding=\"max_length\",max_length=512)\n",
    "transform=partial(transform,tokenizer)\n",
    "dataset.set_transform(transform)\n",
    "# from torchmultimodal.models.flava.model import flava_model_for_classification\n",
    "# model = flava_model_for_classification(num_classes=len(vocab)).cuda()\n",
    "with open(\"./vocabs/answers_textvqa_8k.txt\") as f:\n",
    "  vocab = [answer.strip() for answer in f.readlines()]\n",
    "print(\"UNK index\", vocab[0])\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "def test_model(loader):\n",
    "  pred = []\n",
    "  label = []\n",
    "  for batch in tqdm(loader):\n",
    "    out = model(text = batch[\"input_ids\"].cuda(), image = batch[\"image\"].cuda(), labels = batch[\"answers\"].cuda()).logits\n",
    "    idxs = torch.argmax(out, -1)\n",
    "    pred += [vocab[idx.item()] for idx in idxs]\n",
    "    label += [vocab[batch['answers'][i].item()] for i in range(batch['input_ids'].shape[0])]\n",
    "  print(\"Val Acc\", len([i for i in range(len(pred)) if pred[i] == label[i] if label[i] != '<unk>'])/len(pred))\n",
    "  return pred, label\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "BATCH_SIZE = 64\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "class TextVQA(Dataset):\n",
    "    def __init__(self, inner):\n",
    "        super().__init__()\n",
    "        self.inner = inner\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.inner[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inner)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(TextVQA(dataset[\"train\"]), batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(TextVQA(dataset[\"validation\"]), batch_size=BATCH_SIZE)\n",
    "zero_count = 0\n",
    "for item in dataset['validation']:\n",
    "   if item['answers'] == 0:\n",
    "    zero_count += 1\n",
    "  \n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# epochs = 30\n",
    "# for _ in range(epochs):\n",
    "#   for idx, batch in tqdm(enumerate(train_dataloader)):\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(text = batch[\"input_ids\"].cuda(), image = batch[\"image\"].cuda(), labels = batch[\"answers\"].cuda())\n",
    "#     loss = out.loss\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if idx % 1000 == 0:\n",
    "#         model.eval()\n",
    "#         print(f\"Loss at step {idx} = {loss}\")\n",
    "#         test_model(val_dataloader)\n",
    "#         model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_to_count = defaultdict(int)\n",
    "for ans in item[\"answers\"]:\n",
    "  ans_to_count[ans] += 1\n",
    "max_value = max(ans_to_count, key=ans_to_count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5084"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_count/len(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import torch.nn as nn \n",
    "from datasets import load_dataset\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import warnings\n",
    "import sys \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# num_gpus = int(sys.argv[1])\n",
    "log_file = open(\"log.txt\", \"w\")\n",
    "with open(\"./vocabs/answers_textvqa_8k.txt\") as f:\n",
    "    VOCAB = [answer.strip() for answer in f.readlines()]\n",
    "class TextVQADataset(Dataset):\n",
    "    def __init__(self, inner):\n",
    "        super().__init__()\n",
    "        self.inner = inner\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.inner[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inner)\n",
    "class TextVQA(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                                       padding=\"max_length\",\n",
    "                                                       max_length=128)\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                                       padding=\"max_length\",\n",
    "                                                       max_length=128)\n",
    "        with open(\"./vocabs/answers_textvqa_8k.txt\") as f:\n",
    "            self.vocab = [answer.strip() for answer in f.readlines()]\n",
    "\n",
    "        self.answer_to_idx = {}\n",
    "        for idx, entry in enumerate(self.vocab):\n",
    "            self.answer_to_idx[entry] = idx\n",
    "\n",
    "        self.dataset = load_dataset(\"textvqa\")\n",
    "\n",
    "        transform = partial(self._transform, self.tokenizer)\n",
    "        self.dataset.set_transform(transform)\n",
    "\n",
    "        train_dataset = TextVQADataset(self.dataset[\"train\"])\n",
    "        val_dataset = TextVQADataset(self.dataset[\"validation\"])\n",
    "\n",
    "        self.train_dataset = DataLoader(train_dataset,\n",
    "                                        batch_size=self.batch_size,\n",
    "                                        num_workers=4*num_gpus,\n",
    "                                        pin_memory=True,\n",
    "                                        shuffle=True)\n",
    "        self.val_dataset = DataLoader(val_dataset,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      num_workers=4*num_gpus,\n",
    "                                      pin_memory=True)\n",
    "\n",
    "        self.model = FLAVAModel(num_classes=len(self.vocab))\n",
    "\n",
    "    def _transform(self, tokenizer, input):\n",
    "        batch = {}\n",
    "        image_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                              transforms.Resize([224,224])])\n",
    "        image = image_transform(input[\"image\"][0].convert(\"RGB\"))\n",
    "        batch[\"image\"] = [image]\n",
    "\n",
    "        tokenized = tokenizer(input[\"question\"],\n",
    "                              return_tensors='pt',\n",
    "                              padding=\"max_length\",\n",
    "                              max_length=128)\n",
    "        batch.update(tokenized)\n",
    "\n",
    "        ans_to_count = defaultdict(int)\n",
    "        for ans in input[\"answers\"][0]:\n",
    "            ans_to_count[ans] += 1\n",
    "        max_value = max(ans_to_count, key=ans_to_count.get)\n",
    "        ans_idx = self.answer_to_idx.get(max_value, 0)\n",
    "        batch[\"answers\"] = torch.as_tensor([ans_idx])\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataset\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dataset\n",
    "\n",
    "class FLAVAModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained('/home/ubuntu/models--microsoft--git-base-coco/snapshots/a13141da42abd4a8cbf283601a8104265f537cee')\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, text, image, labels):\n",
    "        logits = self.classifier(self.model(input_ids=text,\n",
    "                                             pixel_values=image).last_hidden_state.mean(1))\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self(text=batch[\"input_ids\"],\n",
    "                   image=batch[\"image\"],\n",
    "                   labels=batch[\"answers\"])\n",
    "        loss = out['loss']\n",
    "        self.log('TRAIN_LOSS', loss, \n",
    "                 on_step=False,\n",
    "                 on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self(text=batch[\"input_ids\"],\n",
    "                   image=batch[\"image\"],\n",
    "                   labels=batch[\"answers\"])\n",
    "        loss = out['loss']\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "\n",
    "        preds = torch.argmax(out['logits'], dim=-1)\n",
    "        acc = ((preds == batch[\"answers\"]) & (batch[\"answers\"] != 0)).float().mean()\n",
    "        if batch_idx == 0:\n",
    "            tb = self.logger.experiment\n",
    "            log_file.write(f'Epoch: {self.trainer.current_epoch}------------------------------------\\n')\n",
    "            for pred_idx, answer_idx in zip(preds, batch['answers']):\n",
    "                log_file.write(f'{VOCAB[pred_idx]} vs {VOCAB[answer_idx]}\\n')\n",
    "                log_file.write('\\n')\n",
    "\n",
    "        self.log('val_acc', acc, on_epoch=True)   \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=5e-6)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ubuntu/models--microsoft--git-base-coco/snapshots/a13141da42abd4a8cbf283601a8104265f537cee were not used when initializing GitModel: ['output.weight', 'output.bias']\n",
      "- This IS expected if you are initializing GitModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GitModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "flava_model = FLAVAModel(num_classes=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flava_model.load_from_checkpoint(\"tb-logger/debug/version_6/checkpoints/best_model.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
